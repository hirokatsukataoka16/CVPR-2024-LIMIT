

<!DOCTYPE html>
<html>
<head>
	<title>CVPR 2024 Workshop LIMIT</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
    .rounded-img-dark {
        border-radius: 50%;
    }
    .table-wrapper table td {
        padding: 10px;
        text-align: center;
    }
    a:link {
        color: green;
    }
    a:visited {
        color: purple;
    }
    a:hover {
        color: red;
    }
    a:active {
        color: yellow;
    }
</style>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>


<h1 align="center" style="font-size: 24pt;"><b>CVPR 2024 Workshop on<br/>Representation Learning with Very Limited Images</b></h1>
<h1 align="center" style="font-size: 16pt;"><b>-Zero-shot, Unsupervised, and Synthetic Learning in the Era of Big Models-</b></h1><br/>
<h1 align="center" style="font-size: 16pt;"><b>June 18th (PM), 2024 at Summit 324</b></h1>

<center>
    <font color="#c7254e"><b>&lt;Related Workshops by Organizers&gt;</b></font><br>
    <section class="delta">
        <div class="container">
            <a href="https://lsfsl.net/limit23/" target="_blank"><button class="btn btn-gray">ICCV 2023 LIMIT</button></a>
            <a href="https://bigmac-vision.github.io/" target="_blank"><button class="btn btn-gray">ICCV 2023 BigMAC</button></a>
            <a href="https://sslwin.org/" target="_blank"><button class="btn btn-gray">ECCV 2020/2022/2024 SSLWIN</button></a>
            <a href="http://www.lsfsl.net/ws/" target="_blank"><button class="btn btn-gray">ICCV 2019 MDALC</button></a>
        </div>
    </section>
</center>

<br>
<h2>The 2nd Workshop on LIMIT</h2>
<p>
We propose the 2nd workshop on “Representation Learning with Very Limited Images: Zero-shot, Unsupervised, and Synthetic Learning in the Era of Big Models” in conjunction with CVPR 2024. At this very moment, the era of ‘foundation models’ heavily relies on a huge amount (>100M-order data) of samples inside of a training dataset. We have witnessed that this kind of large-scale dataset tends to incur ethical issues such as societal bias, copyright, and privacy, due to the uncontrollable big data. On the other hand, the setting of very limited data such as self-supervised learning with a single image or synthetic pre-training with generated images are free of the typical issues. Efforts to train visual/multi-modal models on very limited data resources have emerged independently from various academic and industry communities around the world. This workshop aims to bring together these various communities to form a collaborative effort and find brave new ideas.
</p>

<h2>Broader Impact</h2>

It was an established fact that learning representations were acquired by “human-annotated labels” from “large amount of real data”. The trained models were additionally fine-tuned and applied to each visual task. However, recent problems with large-scale datasets are: (i) biased datasets could lead to e.g., gender and racial discrimination, (ii) suspension of public dataset access due to offensive labels, (iii) ethically problematic images are mixed in a large-scale dataset. As long as a large-scale dataset consisting of real images are used, the situation is endlessly problematic. Here, a recent study reveals that a learning strategy even with very few real images [1] and supervision from a mathematical formula [2] successfully acquires a learned representation of how to see the real world. Moreover, a pre-trained model with artificially generated data outperformed ImageNet-21k pre-training [3] and was found to acquire higher robustness [4]. Thus, it is clear that self-supervised learning (SSL), formula-driven supervised learning (FDSL), and synthetic training in the very limited data setting can also develop a DNN model with high accuracy and safety. Moreover, in the era of foundation models, while these critical issues remain unresolved, there’s growing attention on how pre-training can be achieved with very limited data, whether it’s possible with synthetic images or generative models without any real images, and how adaptation can be carried out using zero/one/few-shot or very limited data. Although these topics have not yet attracted much attention in the computer vision field, these research topics must be focused on since they are expected to be a means to replace learning with real data and resolve ethical issues in the future.

<ul>
  <li>[1] Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi, "A critical analysis of self-supervision, or what we can learn from a single image," ICLR 2020. <a href="https://arxiv.org/abs/1904.13132" target="_blank">[PDF]</a> <a href="https://github.com/yukimasano/single_img_pretraining" target="_blank">[Project]</a></li>
  <li>[2] Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada, Nakamasa Inoue, Akio Nakamura, Yutaka Satoh, "Pre-training without Natural Images," IJCV 2022. <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01555-8.pdf" target="_blank">[PDF]</a> <a href="https://hirokatsukataoka16.github.io/Pretraining-without-Natural-Images/" target="_blank">[Project]</a></li>
  <li>[3] Hirokatsu Kataoka, Ryo Hayamizu, Ryosuke Yamada, Kodai Nakashima, Sora Takashima, Xinyu Zhang, Edgar Josafat Martinez-Noriega, Nakamasa Inoue, Rio Yokota, "Replacing Labeled Real-image Datasets with Auto-generated Contours," CVPR 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kataoka_Replacing_Labeled_Real-Image_Datasets_With_Auto-Generated_Contours_CVPR_2022_paper.html" target="_blank">[PDF]</a> <a href="https://hirokatsukataoka16.github.io/Replacing-Labeled-Real-Image-Datasets/" target="_blank">[Project]</a></li>
  <li>[4] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, Jacob Steinhardt, "PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures," CVPR 2022. <a href="https://arxiv.org/abs/2112.05135" target="_blank">[PDF]</a> <a href="https://github.com/andyzoujm/pixmix" target="_blank">[Project]</a></li>
</ul>
* The listed papers are proposed by organizers or invited speakers

<h2>Invited Talk 1: Phillip Isola (MIT)</h2>
<center>
    <img src="./img/photo_of_me_korea.jpeg" style="width: 35%;"/>
</center>
<br>
<font color="#c7254e"><b>Title: N=0: Learning Vision with Zero Visual Data</b></font><br>
<b>Bio:</b>Phillip Isola is the Class of 1948 Career Development associate professor in EECS at MIT. He studies computer vision, machine learning, robotics, and AI. He completed his Ph.D. in Brain & Cognitive Sciences at MIT, and has since spent time at UC Berkeley, OpenAI, and Google Research. His work has particularly impacted generative AI and self-supervised representation learning. Dr. Isola's research has been recognized by a Google Faculty Research Award, a PAMI Young Researcher Award, a Samsung AI Researcher of the Year Award, a Packard Fellowship, and a Sloan Fellowship. His teaching has been recognized by the Ruth and Joel Spira Award for Distinguished Teaching. His current research focuses on trying to scientifically understand human-like intelligence.  
(Refer from: <a href="http://web.mit.edu/phillipi/www/bio.html" target="_blank">http://web.mit.edu/phillipi/www/bio.html</a>)
<br>

<h2>Invited Talk 2: Zeynep Akata (Helmholtz Munich/TUM)</h2>
<center>
    <img src="./img/zeynep-akata.jpg" style="width: 35%;"/>
</center>
<br>
<b>Bio:</b>Zeynep Akata is a Liesel Beckmann Distinguished professor of Computer Science at Technical University of Munich and the director of the Institute for Explainable Machine Learning at Helmholtz Munich. After completing her PhD at the INRIA Rhone Alpes with Prof Cordelia Schmid (2014), she worked as a post-doctoral researcher at the Max Planck Institute for Informatics with Prof Bernt Schiele (2014-17) and at University of California Berkeley with Prof Trevor Darrell (2016-17) and as an assistant professor at the University of Amsterdam with Prof Max Welling (2017-19). Before moving to Munich in 2024, she was a professor of computer science (W3) within the Cluster of Excellence Machine Learning at the University of Tübingen. She received a Lise-Meitner Award for Excellent Women in Computer Science from Max Planck Society in 2014, a young scientist honour from the Werner-von-Siemens-Ring foundation in 2019, an ERC-2019 Starting Grant from the European Commission, The DAGM German Pattern Recognition Award in 2021, The ECVA Young Researcher Award in 2022 and the Alfried Krupp Award in 2023. Her research interests include multimodal learning and explainable AI. (Refer from: <a href="https://www.eml-unitue.de/people/zeynep-akata" target="_blank">https://www.eml-unitue.de/people/zeynep-akata</a>)
<br>

<br>
<h2>Program (Date: June 18th PM, Room: Summit 324)</h2>
<ul>
  <li>13:30 - 13:40: Opening remarks (Talk: <a href="http://hirokatsukataoka.net/" target="_blank">Hirokatsu Kataoka</a>)</li>
  <li>13:40 - 14:20: Invited Talk 1 (Talk: <a href="http://web.mit.edu/phillipi/" target="_blank">Phillip Isola</a>, Chair: <a href="https://yukimasano.github.io/" target="_blank">Yuki M. Asano</a>)</li>
  <li>14:20 - 15:00: Oral Session (Chair: <a href="https://www.ryosuke-yamada.net/" target="_blank">Ryosuke Yamada</a>)</li>
  <li>15:00 - 15:40: Coffee Break</li>
  <li>15:40 - 16:20: Invited Talk 2 (Talk: <a href="https://www.eml-unitue.de/people/zeynep-akata" target="_blank">Zeynep Akata</a>, Chair: <a href="https://chrirupp.github.io/" target="_blank">Christian Rupprecht</a>)</li>
  <li>16:20 - 16:30: Short Break</li>
  <li>16:30 - 17:30: Poster Session (Room: Arch Building 4E, Chair: <a href="" target="_blank">Erika Mori</a>)</li>
  <li>* In-person only</li>
</ul>

<h2>Oral Session (14:20 - 15:00; Room: Summit 324)</h2>
Oral presenters have 10 minutes including questions. Oral presenters are also required to present at the poster session.
<ul>
  <li>Oral/Poster 1 [Panel#74]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Zhang_i-MAE_Are_Latent_Representations_in_Masked_Autoencoders_Linearly_Separable_CVPRW_2024_paper.html" target="_blank">i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable?</a></ul>
    <ul>Kevin Zhang (Peking University), Zhiqiang Shen (MBZUAI)</ul>
  </li>
  <li>Oral/Poster 2 [Panel#75]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Fan_POPE_6-DoF_Promptable_Pose_Estimation_of_Any_Object_in_Any_CVPRW_2024_paper.html" target="_blank">POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</a></ul>
    <ul>Zhiwen Fan (University of Texas at Austin), Panwang Pan (ByteDance Inc), Peihao Wang (University of Texas at Austin), Yifan Jiang (University of Texas at Austin), Dejia Xu (University of Texas at Austin), Zhangyang Wang (University of Texas at Austin)</ul>
  </li>
  <li>Oral/Poster 3 [Panel#76]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Soni_Federated_Learning_with_a_Single_Shared_Image_CVPRW_2024_paper.html" target="_blank">Federated Learning with a Single Shared Image</a></ul>
    <ul>Sunny Soni (Universiteit van Amsterdam), Aaqib  Saeed (Eindhoven University of Technology), Yuki M Asano (University of Amsterdam)</ul>
  </li>
  <li>Oral/Poster 4 [Panel#77]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Xing_Vision-Language_Pseudo-Labels_for_Single-Positive_Multi-Label_Learning_CVPRW_2024_paper.html" target="_blank">Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning</a></ul>
    <ul>Xin Xing (University of Nebraska Omaha), Zhexiao Xiong (Washington University in St. Louis), Abby Stylianou (Saint Louis University), Srikumar Sastry (Washington University in St. Louis), Liyu Gong (Oracle Inc), Nathan Jacobs (Washington University in St. Louis)</ul>
  </li>
</ul>

<h2>Poster Session (16:30 - 17:30; Room: Arch Building 4E)</h2>
Posters will be 84” x 42” = 213 cm x 107cm (WxH, aspect ratio 2:1, landscape format). If you want to use the CVPR logo on your poster, you can download them as a zip file <a href="https://drive.google.com/drive/folders/1soCIrRHehVYKxJugJq4_UDvN-B21D1GS?usp=sharing" target="_blank">here</a> (Refer from: <a href="https://cvpr.thecvf.com/Conferences/2024/PosterPrintingInformation" target="_blank">CVPR Official Website</a>)
<ul>
  <li>Poster 5 [Panel#78]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Shi_Data-free_Model_Fusion_with_Generator_Assistants_CVPRW_2024_paper.html" target="_blank">Data-free Model Fusion with Generator Assistants</a></ul>
    <ul>Luyao Shi (IBM Research), Prashanth Vijayaraghavan (IBM Research); Ehsan  Degan (IBM Research)</ul>
  </li>
  <li>Poster 6 [Panel#79]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Aygun_Enhancing_2D_Representation_Learning_with_a_3D_Prior_CVPRW_2024_paper.html" target="_blank">Enhancing 2D Representation Learning with a 3D Prior</a></ul>
    <ul>Mehmet Aygün (University of Edinburgh), Prithviraj  Dhar (Meta), Zhicheng Yan (Meta Reality Labs), Oisin Mac Aodha (University of Edinburgh); Rakesh Ranjan (Meta)</ul>
  </li>
  <li>Poster 7 [Panel#80]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Hirohashi_Prompt_Learning_with_One-Shot_Setting_based_Feature_Space_Analysis_in_CVPRW_2024_paper.html" target="_blank">Prompt Learning with One-Shot Setting based Feature Space Analysis in Vision-and-Language Models</a></ul>
    <ul>Yuki HIROHASHI (OMRON Corp.), Tsubasa Hirakawa (Chubu University), Takayoshi Yamashita (Chubu University), Hironobu Fujiyoshi (Chubu Univ.)</ul>
  </li>
  <li>Poster 8 [Panel#81]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Feinglass_Eyes_of_a_Hawk_and_Ears_of_a_Fox_Part_CVPRW_2024_paper.html" target="_blank">`Eyes of a Hawk and Ears of a Fox’: Part Prototype Network for Generalized Zero-Shot Learning</a></ul>
    <ul>Joshua Feinglass (Arizona State University), Jayaraman J. Thiagarajan (Lawrence Livermore National Laboratory), Rushil Anirudh (Amazon); T.S. Jayram (LLNL), Yezhou Yang (Arizona State University)</ul>
  </li>
  <li>Poster 9 [Panel#82]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Majurski_A_Method_of_Moments_Embedding_Constraint_and_its_Application_to_CVPRW_2024_paper.html" target="_blank">A Method of Moments Embedding Constraint and its Application to Semi-Supervised Learning</a></ul>
    <ul>Michael Majurski (NIST), Sumeet Menon (University of Maryland, Baltimore County), Parniyan Favardin (University of Miami), David R Chapman (University of Miami)</ul>
  </li>
  <li>Poster 10 [Panel#83]: 
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Khandelwal_PromptSync_Bridging_Domain_Gaps_in_Vision-Language_Models_through_Class-Aware_Prototype_CVPRW_2024_paper.html" target="_blank">PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination</a></ul>
    <ul>Anant Khandelwal (IIT Delhi)</ul>
  </li>
  <li>Poster 11 [Invited poster 1; ICCV 2023 accepted paper; Panel#84]:
    <ul><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shinoda_SegRCDB_Semantic_Segmentation_via_Formula-Driven_Supervised_Learning_ICCV_2023_paper.html" target="_blank">SegRCDB: Semantic Segmentation via Formula-Driven Supervised Learning</a></ul>
    <ul>Risa Shinoda (AIST/University of Kyoto), Ryo Hayamizu (AIST/TDU), Kodai Nakashima (AIST/University of Tsukuba), Nakamasa Inoue (AIST/TokyoTech), Rio Yokota (AIST/TokyoTech), Hirokatsu Kataoka (AIST)</ul>
  </li>
  <li>Poster 12 [Invited poster 2; CVPR 2024 accepted paper; Panel#85]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Szymanowicz_Splatter_Image_Ultra-Fast_Single-View_3D_Reconstruction_CVPR_2024_paper.html" target="_blank">Splatter Image: Ultra-Fast Single-View 3D Reconstruction</a></ul>
    <ul>Stanislaw Szymanowicz Christian Rupprecht Andrea Vedaldi (University of Oxford)</ul>
  </li>
  <li>Poster 13 [Invited poster 3; CVPR 2024 accepted paper; Panel#86]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Knobel_Learning_to_Count_without_Annotations_CVPR_2024_paper.html" target="_blank">Learning to Count without Annotations</a></ul>
    <ul>Lukas Knobel (University of Amsterdam), Tengda Han (University of Oxford), Yuki M. Asano (University of Amsterdam)</ul>
  </li>
  <li>Poster 14 [Invited poster 4; CVPR 2024 accepted paper; Panel#87]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Dorkenwald_PIN_Positional_Insert_Unlocks_Object_Localisation_Abilities_in_VLMs_CVPR_2024_paper.html" target="_blank">PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</a></ul>
    <ul>Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano(University of Amsterdam)</ul>
  </li>
  <li>Poster 15 [Invited poster 5; CVPR 2024 accepted paper; Panel#88]:
    <ul><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chiche_Pre-training_Vision_Models_with_Mandelbulb_Variations_CVPR_2024_paper.html" target="_blank">Pre-training Vision Models with Mandelbulb Variations</a></ul>
    <ul>Benjamin N. Chiche, Yuto Horikawa, Ryo Fujita (Rist Inc.)</ul>
  </li>
  
</ul>

<h2>Paper Submission / Call For Papers</h2>

<ul>
  <li><b>Guide for Authors.</b> We invite original research papers. All submissions should be anonymized, formatted according to the template of CVPR 2024 (Recommended: 4-8 pages excluding references): <a href="https://cvpr.thecvf.com/Conferences/2024/CallForPapers" target="_blank">https://cvpr.thecvf.com/Conferences/2024/CallForPapers</a>.
  </li>
  <li><b>Microsoft CMT Site.</b> Please submit your paper here: <a href="https://cmt3.research.microsoft.com/CVPRLIMIT2024" target="_blank">https://cmt3.research.microsoft.com/CVPRLIMIT2024</a></li>
  <ul>
    <li>The accepted papers will be published on the CVF proceedings: <a href="https://openaccess.thecvf.com/menu" target="_blank">https://openaccess.thecvf.com/menu</a></li>
  </ul>
  <br>
  <li>{Self, Semi, Weakly, Un}-supervised learning with very limiteda data</li>
  <li>{Zero, One, Few}-shot learning with very limited data</li>
  <li>Synthetic training with computer graphics</li>
  <li>Model training from generative models and/or formula-generated data</li>
  <li>Training convnets, vision transformers, and more models with relatively fewer resources, Vision+X and multi-modality to accelerate a learning efficiency</li>
  <li>New datasets, benchmarks, and meta analysis</li>
  <li>Brave new ideas related to above-mentioned topics or create a novel field</li>
</ul>
<br>

<h2>Important Dates</h2>
<ul>
  <li>Submission deadline: <b>March 21st, 2024</b></li>
  <li>Notification:          <b>April 4th, 2024</b></li>
  <li>Camera-ready deadline: <b>April 11th, 2024</b></li>
  <li># All dates are following Pacific Time (PT).</li>

</ul>

<br>

<h2>Organizers</h2>

<div class="table-wrapper" style="width:100%">
        <table cellpadding="0" cellspacing="0">
            <tr>
                <td><img class="rounded-img-dark" height="150px" src="img/kataoka.png"><br><a href="http://hirokatsukataoka.net/" target="_blank">Hirokatsu Kataoka<br>AIST</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/asano.png"><br><a href="https://yukimasano.github.io/" target="_blank">Yuki M. Asano<br>University of Amsterdam</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/rupprecht.jpg"><br><a href="https://chrirupp.github.io/" target="_blank">Christian Rupprecht<br>University of Oxford</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/yokota.png"><br><a href="https://www.rio.gsic.titech.ac.jp/en/index.html" target="_blank">Rio Yokota<br>Tokyo Tech/AIST</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/inoue.png"><br><a href="http://mmai.tech" target="_blank">Nakamasa Inoue<br>Tokyo Tech/AIST</a></td>
            </tr>
            <tr>
                <td><img class="rounded-img-dark" height="150px" src="img/hendrycks.png"><br><a href="https://people.eecs.berkeley.edu/~hendrycks/">Dan Hendrycks<br>Center for AI Safety</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/boix.png"><br><a href="https://www.mit.edu/~xboix/" target="_blank">Xavier Boix<br>Fujitsu Research</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/baradad.jpg"><br><a href="https://mbaradad.github.io/" target="_blank">Manel Baradad<br>MIT</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/anderson.png"><br><a href="https://scholar.google.com/citations?user=UEKWqIoAAAAJ&hl=en" target="_blank">Connor Anderson<br>BYU</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/nakamura.png"><br><a href="https://ryoo-portfolio.netlify.app/" target="_blank">Ryo Nakamura<br>Fukuoka University/AIST</a></td>
            </tr>
            <tr>
                <td><img class="rounded-img-dark" height="150px" src="img/yamada.png"><br><a href="https://www.ryosuke-yamada.net/" target="_blank">Ryosuke Yamada<br>University of Tsukuba/AIST</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/shinoda.png"><br><a href="https://sites.google.com/view/risashinoda/home" target="_blank">Risa Shinoda<br>Kyoto University/AIST</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/tadokoro.jpg"><br><a href="https://www.linkedin.com/in/ryu-tadokoro-87a204294/" target="_blank">Ryu Tadokoro<br>Tohoku University/AIST</a></td>
                <td><img class="rounded-img-dark" height="150px" src="img/mori.jpg"><br><a href="" target="_blank">Erika Mori<br>Keio University/AIST</a></td>
            </tr>
        </table>
</div>

<br><br><br>
<script type="text/javascript" src="./footer.js"></script>
</body></html>